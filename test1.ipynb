{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71356018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import torch\n",
    "from config.config import load_nested_params\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def LoadModel(model_path='../../model/bge-large-zh-v1.5'):\n",
    "    \n",
    "    base_dir = os.getcwd()\n",
    "    model_dir = load_nested_params(\"model\",\"embedding\",\"path\")\n",
    "    model_name = load_nested_params(\"model\",\"embedding\",\"name\")\n",
    "    if model_dir is None:\n",
    "        model_path = model_name\n",
    "    else:\n",
    "        model_path = os.path.join(base_dir, model_dir, model_name)\n",
    "    \n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_path,\n",
    "                                       model_kwargs=model_kwargs,\n",
    "                                       encode_kwargs=encode_kwargs)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b6e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''本地知识库的RAG检索模型类'''\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import backend.model.Embedding as Embedding\n",
    "from backend.config.config import load_nested_params\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    PyPDFLoader,\n",
    "    MHTMLLoader,\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "\n",
    "import re\n",
    "from typing import Generator\n",
    "\n",
    "class OptimizedCleaner:\n",
    "    # 预编译所有正则（只需一次）\n",
    "    _regex = {\n",
    "        # 1. 移除所有空白符（含换行、制表符、零宽空格等）\n",
    "        'normalize_whitespace': re.compile(r'[\\s\\u200B\\u200C\\u200D\\uFEFF]+', re.UNICODE),\n",
    "        # 2. 移除ASCII控制字符（含DEL）\n",
    "        'remove_ascii_controls': re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]', re.UNICODE),\n",
    "        # 3. 移除Unicode私有区字符（U+E000-U+F8FF）\n",
    "        'remove_private_use': re.compile(r'[\\uE000-\\uF8FF]', re.UNICODE),\n",
    "        # 4. 移除常见特殊符号（可按需扩展）\n",
    "        'remove_symbols': re.compile(r'[\\u2022\\u2192\\u25A0\\u25CF\\u25CB]', re.UNICODE),\n",
    "        # 5. 合并连续空格为单个空格\n",
    "        'compact_spaces': re.compile(r' {2,}'),\n",
    "        # 6. 移除重复的特殊符号（如连续*、_等）\n",
    "        'remove_repeated_symbols': re.compile(r'([^\\w\\s])\\1{2,}'),\n",
    "        # 7. 移除目录占位符（连续点号）\n",
    "        'remove_toc_placeholders': re.compile(r'\\.{3,}|\\s+\\.\\s+'),\n",
    "        # 8. 移除重复标点符号\n",
    "        'remove_repeated_punctuations': re.compile(r'([. .,!?;:])')\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def process_stream(cls, document_stream: Generator) -> Generator:\n",
    "        \"\"\"流式处理文档\"\"\"\n",
    "        for doc in document_stream:\n",
    "            text = doc.page_content\n",
    "            \n",
    "            # 按顺序清洗\n",
    "            text = cls._regex['normalize_whitespace'].sub(' ', text)        # 归一化空白符\n",
    "            text = cls._regex['remove_ascii_controls'].sub('', text)        # 移除ASCII控制字符\n",
    "            text = cls._regex['remove_private_use'].sub('', text)          # 移除私有区字符\n",
    "            text = cls._regex['remove_symbols'].sub('', text)              # 移除特殊符号\n",
    "            text = cls._regex['remove_repeated_symbols'].sub(r'\\1', text)  # 移除重复符号\n",
    "            text = cls._regex['compact_spaces'].sub(' ', text)             # 合并空格\n",
    "            text = cls._regex['remove_toc_placeholders'].sub(' ', text) # 移除目录占位符（连续点号）\n",
    "            text = cls._regex['remove_repeated_punctuations'].sub(r'\\1', text) # 移除重复标点符号\n",
    "            # 去除首尾空白\n",
    "            text = text.strip()\n",
    "            \n",
    "            # 仅保留非空文档\n",
    "            if text:\n",
    "                doc.page_content = text\n",
    "                yield doc\n",
    "\n",
    "\n",
    "\n",
    "# 检索模型\n",
    "class Retrieve_model(object):\n",
    "\n",
    "    _retriever: VectorStoreRetriever\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_model = Embedding.LoadModel()\n",
    "\n",
    "        self.knowledge_path = load_nested_params(\"knowledge-path\")\n",
    "        if not os.path.exists(self.knowledge_path):\n",
    "            os.makedirs(self.knowledge_path)\n",
    "        \n",
    "        self.faiss_path = load_nested_params(\"faiss-path\")\n",
    "        \n",
    "        #检查self.faiss_path文件夹是否为空\n",
    "        if not os.listdir(self.faiss_path):\n",
    "            print(f\"{self.faiss_path}文件夹为空\")\n",
    "            self.build_vectorstore()\n",
    "        \n",
    "        # 加载向量库\n",
    "        self.vectorstore = FAISS.load_local(self.faiss_path, embeddings=self.embedding_model,allow_dangerous_deserialization=True)\n",
    "        print(\"加载向量库成功\")\n",
    "        # 将向量存储转换为检索器，设置检索参数 k 为 6，即返回最相似的 6 个文档\n",
    "        self._retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "    \n",
    "    # 加载文件\n",
    "    def load_file(self,file_path):\n",
    "\n",
    "        # 加载PDF文件\n",
    "        pdf_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.pdf\",\n",
    "            loader_cls=PyPDFLoader,\n",
    "            silent_errors=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        pdf_docs = pdf_loader.load()\n",
    "\n",
    "        # 加载Word文件\n",
    "        docx_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.docx\",\n",
    "            loader_cls=UnstructuredWordDocumentLoader,\n",
    "            silent_errors=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        docx_docs = docx_loader.load()\n",
    "\n",
    "        # 加载txt文件\n",
    "        txt_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader,\n",
    "            silent_errors=True,\n",
    "            loader_kwargs={\"autodetect_encoding\": True},\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        txt_docs = txt_loader.load()\n",
    "\n",
    "        # 加载csv文件\n",
    "        csv_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.csv\",\n",
    "            loader_cls=CSVLoader,\n",
    "            silent_errors=True,\n",
    "            loader_kwargs={\"autodetect_encoding\": True},\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        csv_docs = csv_loader.load()\n",
    "\n",
    "        # 加载html文件\n",
    "        html_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.html\",\n",
    "            loader_cls=UnstructuredHTMLLoader,\n",
    "            silent_errors=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        html_docs = html_loader.load()\n",
    "\n",
    "        mhtml_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.mhtml\",\n",
    "            loader_cls=MHTMLLoader,\n",
    "            silent_errors=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        mhtml_docs = mhtml_loader.load()\n",
    "\n",
    "        # 加载markdown文件\n",
    "        markdown_loader = DirectoryLoader(\n",
    "            file_path,\n",
    "            glob=\"**/*.md\",\n",
    "            loader_cls=UnstructuredMarkdownLoader,\n",
    "            silent_errors=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        markdown_docs = markdown_loader.load()\n",
    "        \n",
    "        print(f\"加载文档完成，共加载了{len(pdf_docs)}个pdf文档,{len(docx_docs)}个docx文档, {len(txt_docs)}个txt文档, {len(csv_docs)}个csv文档, {len(html_docs)}个html文档, {len(mhtml_docs)}个mhtml文档, {len(markdown_docs)}个markdown文档\")\n",
    "        # 合并文档\n",
    "        docs = (\n",
    "            pdf_docs\n",
    "            + docx_docs\n",
    "            + txt_docs\n",
    "            + csv_docs\n",
    "            + html_docs\n",
    "            + mhtml_docs\n",
    "            + markdown_docs\n",
    "        )\n",
    "        # 清洗文档\n",
    "        docs = list(OptimizedCleaner.process_stream(docs))\n",
    "\n",
    "        # 分割文档,使其按句号分割\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \"。\", \"！\", \"？\"],\n",
    "            chunk_size=500, chunk_overlap=100\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        print(f\"分割文档完成，共分割了{len(splits)}个块\")\n",
    "\n",
    "        for i, doc in enumerate(splits[:3]):\n",
    "            print(f\"文档 {i}:\\n{doc.page_content[:200]}\\n\")\n",
    "\n",
    "        return splits\n",
    "\n",
    "\n",
    "    # 创建向量库\n",
    "    def build_vectorstore(self,):\n",
    "        \n",
    "        splits=self.load_file(self.knowledge_path)\n",
    "        \n",
    "        try:\n",
    "            vectorstore = None\n",
    "            batch_size = 10  \n",
    "            faiss_temp_path = self.faiss_path + \"_temp\"  # 临时保存路径\n",
    "\n",
    "            for i in range(0, len(splits), batch_size):\n",
    "                batch = splits[i:i+batch_size]\n",
    "                \n",
    "                # 如果是第一个批次，创建新向量库并保存\n",
    "                if i == 0:\n",
    "                    vectorstore = FAISS.from_documents(documents=batch, embedding=self.embedding_model)\n",
    "                    vectorstore.save_local(faiss_temp_path)\n",
    "                    print(f\"初始批次已保存至临时路径: {faiss_temp_path}\")\n",
    "                    \n",
    "                # 后续批次：加载已有库 -> 添加新数据 -> 覆盖保存\n",
    "                else:\n",
    "                    vectorstore = FAISS.load_local(\n",
    "                        faiss_temp_path,\n",
    "                        embeddings=self.embedding_model,\n",
    "                        allow_dangerous_deserialization=True  # 允许加载不安全的序列化对象\n",
    "                    )  \n",
    "                    \n",
    "                    vectorstore.add_documents(batch)  # 添加新文档\n",
    "                    vectorstore.save_local(faiss_temp_path) # 覆盖保存更新后的库\n",
    "                    print(f\"增量保存第 {i//batch_size} 批数据\")\n",
    "                \n",
    "                # 手动释放内存\n",
    "                del vectorstore\n",
    "                gc.collect()\n",
    "                print(f\"已处理 {min(i + batch_size, len(splits))} 个文档块 (共 {len(splits)})\")\n",
    "\n",
    "            # 全部完成后重命名为正式路径\n",
    "            if os.path.exists(faiss_temp_path):\n",
    "                os.rename(faiss_temp_path, self.faiss_path)\n",
    "                print(f\"向量数据库最终保存至: {self.faiss_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理过程中发生错误: {e}\")\n",
    "            # 清理可能的残留临时文件\n",
    "            if os.path.exists(faiss_temp_path):\n",
    "                shutil.rmtree(faiss_temp_path)\n",
    "            raise\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     retrieve_model=Retrieve_model()\n",
    "#     retrieve_model._retriever.invoke('头痛眼花')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "129399e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载embedding模型成功！\n",
      "./backend/faiss_index_temp文件夹为空\n",
      "加载文档完成，共加载了480个pdf文档,0个docx文档, 0个txt文档, 0个csv文档, 0个html文档, 0个mhtml文档, 0个markdown文档\n",
      "分割文档完成，共分割了1437个块\n",
      "文档 0:\n",
      "内科治疗指南 github 项⽬主⻚：https://github.com/scienceasdf/medical-books 新书下载：https://github.com/scienceasdf/medical-books/releases/latest 年 ⽉ ⽇\n",
      "\n",
      "文档 1:\n",
      "⽬录 ⽬录 插图 表格 第⼀章 呼吸系统疾病 急性上呼吸道感染 . . . . . . . . . . . . . . . . . 急性⽓管-⽀⽓管炎 . . . . . . . . . . . . . . . . 慢性⽀⽓管炎 . . . . . . . . . . . . . . . . . . 慢性阻塞性肺疾病 . . . . . . . . . . . . . . . . . 慢性肺源性⼼\n",
      "\n",
      "文档 2:\n",
      ". . . 肺⾎栓栓塞症 . . . . . . . . . . . . . . . . . . 肺结核病 . . . . . . . . . . . . . . . . . . . ⽀⽓管扩张症 . . . . . . . . . . . . . . . . . . 原发性⽀⽓管肺癌 . . . . . . . . . . . . . . . . . 弥漫性间质性肺病 . . . . . . .\n",
      "\n",
      "初始批次已保存至临时路径: ./backend/faiss_index_temp_temp\n",
      "已处理 10 个文档块 (共 1437)\n",
      "增量保存第 1 批数据\n",
      "已处理 20 个文档块 (共 1437)\n",
      "增量保存第 2 批数据\n",
      "已处理 30 个文档块 (共 1437)\n",
      "增量保存第 3 批数据\n",
      "已处理 40 个文档块 (共 1437)\n",
      "增量保存第 4 批数据\n",
      "已处理 50 个文档块 (共 1437)\n",
      "增量保存第 5 批数据\n",
      "已处理 60 个文档块 (共 1437)\n",
      "增量保存第 6 批数据\n",
      "已处理 70 个文档块 (共 1437)\n",
      "增量保存第 7 批数据\n",
      "已处理 80 个文档块 (共 1437)\n",
      "增量保存第 8 批数据\n",
      "已处理 90 个文档块 (共 1437)\n",
      "增量保存第 9 批数据\n",
      "已处理 100 个文档块 (共 1437)\n",
      "增量保存第 10 批数据\n",
      "已处理 110 个文档块 (共 1437)\n",
      "增量保存第 11 批数据\n",
      "已处理 120 个文档块 (共 1437)\n",
      "增量保存第 12 批数据\n",
      "已处理 130 个文档块 (共 1437)\n",
      "增量保存第 13 批数据\n",
      "已处理 140 个文档块 (共 1437)\n",
      "增量保存第 14 批数据\n",
      "已处理 150 个文档块 (共 1437)\n",
      "增量保存第 15 批数据\n",
      "已处理 160 个文档块 (共 1437)\n",
      "增量保存第 16 批数据\n",
      "已处理 170 个文档块 (共 1437)\n",
      "增量保存第 17 批数据\n",
      "已处理 180 个文档块 (共 1437)\n",
      "增量保存第 18 批数据\n",
      "已处理 190 个文档块 (共 1437)\n",
      "增量保存第 19 批数据\n",
      "已处理 200 个文档块 (共 1437)\n",
      "增量保存第 20 批数据\n",
      "已处理 210 个文档块 (共 1437)\n",
      "增量保存第 21 批数据\n",
      "已处理 220 个文档块 (共 1437)\n",
      "增量保存第 22 批数据\n",
      "已处理 230 个文档块 (共 1437)\n",
      "增量保存第 23 批数据\n",
      "已处理 240 个文档块 (共 1437)\n",
      "增量保存第 24 批数据\n",
      "已处理 250 个文档块 (共 1437)\n",
      "增量保存第 25 批数据\n",
      "已处理 260 个文档块 (共 1437)\n",
      "增量保存第 26 批数据\n",
      "已处理 270 个文档块 (共 1437)\n",
      "增量保存第 27 批数据\n",
      "已处理 280 个文档块 (共 1437)\n",
      "增量保存第 28 批数据\n",
      "已处理 290 个文档块 (共 1437)\n",
      "增量保存第 29 批数据\n",
      "已处理 300 个文档块 (共 1437)\n",
      "增量保存第 30 批数据\n",
      "已处理 310 个文档块 (共 1437)\n",
      "增量保存第 31 批数据\n",
      "已处理 320 个文档块 (共 1437)\n",
      "增量保存第 32 批数据\n",
      "已处理 330 个文档块 (共 1437)\n",
      "增量保存第 33 批数据\n",
      "已处理 340 个文档块 (共 1437)\n",
      "增量保存第 34 批数据\n",
      "已处理 350 个文档块 (共 1437)\n",
      "增量保存第 35 批数据\n",
      "已处理 360 个文档块 (共 1437)\n",
      "增量保存第 36 批数据\n",
      "已处理 370 个文档块 (共 1437)\n",
      "增量保存第 37 批数据\n",
      "已处理 380 个文档块 (共 1437)\n",
      "增量保存第 38 批数据\n",
      "已处理 390 个文档块 (共 1437)\n",
      "增量保存第 39 批数据\n",
      "已处理 400 个文档块 (共 1437)\n",
      "增量保存第 40 批数据\n",
      "已处理 410 个文档块 (共 1437)\n",
      "增量保存第 41 批数据\n",
      "已处理 420 个文档块 (共 1437)\n",
      "增量保存第 42 批数据\n",
      "已处理 430 个文档块 (共 1437)\n",
      "增量保存第 43 批数据\n",
      "已处理 440 个文档块 (共 1437)\n",
      "增量保存第 44 批数据\n",
      "已处理 450 个文档块 (共 1437)\n",
      "增量保存第 45 批数据\n",
      "已处理 460 个文档块 (共 1437)\n",
      "增量保存第 46 批数据\n",
      "已处理 470 个文档块 (共 1437)\n",
      "增量保存第 47 批数据\n",
      "已处理 480 个文档块 (共 1437)\n",
      "增量保存第 48 批数据\n",
      "已处理 490 个文档块 (共 1437)\n",
      "增量保存第 49 批数据\n",
      "已处理 500 个文档块 (共 1437)\n",
      "增量保存第 50 批数据\n",
      "已处理 510 个文档块 (共 1437)\n",
      "增量保存第 51 批数据\n",
      "已处理 520 个文档块 (共 1437)\n",
      "增量保存第 52 批数据\n",
      "已处理 530 个文档块 (共 1437)\n",
      "增量保存第 53 批数据\n",
      "已处理 540 个文档块 (共 1437)\n",
      "增量保存第 54 批数据\n",
      "已处理 550 个文档块 (共 1437)\n",
      "增量保存第 55 批数据\n",
      "已处理 560 个文档块 (共 1437)\n",
      "增量保存第 56 批数据\n",
      "已处理 570 个文档块 (共 1437)\n",
      "增量保存第 57 批数据\n",
      "已处理 580 个文档块 (共 1437)\n",
      "增量保存第 58 批数据\n",
      "已处理 590 个文档块 (共 1437)\n",
      "增量保存第 59 批数据\n",
      "已处理 600 个文档块 (共 1437)\n",
      "增量保存第 60 批数据\n",
      "已处理 610 个文档块 (共 1437)\n",
      "增量保存第 61 批数据\n",
      "已处理 620 个文档块 (共 1437)\n",
      "增量保存第 62 批数据\n",
      "已处理 630 个文档块 (共 1437)\n",
      "增量保存第 63 批数据\n",
      "已处理 640 个文档块 (共 1437)\n",
      "增量保存第 64 批数据\n",
      "已处理 650 个文档块 (共 1437)\n",
      "增量保存第 65 批数据\n",
      "已处理 660 个文档块 (共 1437)\n",
      "增量保存第 66 批数据\n",
      "已处理 670 个文档块 (共 1437)\n",
      "增量保存第 67 批数据\n",
      "已处理 680 个文档块 (共 1437)\n",
      "增量保存第 68 批数据\n",
      "已处理 690 个文档块 (共 1437)\n",
      "增量保存第 69 批数据\n",
      "已处理 700 个文档块 (共 1437)\n",
      "增量保存第 70 批数据\n",
      "已处理 710 个文档块 (共 1437)\n",
      "增量保存第 71 批数据\n",
      "已处理 720 个文档块 (共 1437)\n",
      "增量保存第 72 批数据\n",
      "已处理 730 个文档块 (共 1437)\n",
      "增量保存第 73 批数据\n",
      "已处理 740 个文档块 (共 1437)\n",
      "增量保存第 74 批数据\n",
      "已处理 750 个文档块 (共 1437)\n",
      "增量保存第 75 批数据\n",
      "已处理 760 个文档块 (共 1437)\n",
      "增量保存第 76 批数据\n",
      "已处理 770 个文档块 (共 1437)\n",
      "增量保存第 77 批数据\n",
      "已处理 780 个文档块 (共 1437)\n",
      "增量保存第 78 批数据\n",
      "已处理 790 个文档块 (共 1437)\n",
      "增量保存第 79 批数据\n",
      "已处理 800 个文档块 (共 1437)\n",
      "增量保存第 80 批数据\n",
      "已处理 810 个文档块 (共 1437)\n",
      "增量保存第 81 批数据\n",
      "已处理 820 个文档块 (共 1437)\n",
      "增量保存第 82 批数据\n",
      "已处理 830 个文档块 (共 1437)\n",
      "增量保存第 83 批数据\n",
      "已处理 840 个文档块 (共 1437)\n",
      "增量保存第 84 批数据\n",
      "已处理 850 个文档块 (共 1437)\n",
      "增量保存第 85 批数据\n",
      "已处理 860 个文档块 (共 1437)\n",
      "增量保存第 86 批数据\n",
      "已处理 870 个文档块 (共 1437)\n",
      "增量保存第 87 批数据\n",
      "已处理 880 个文档块 (共 1437)\n",
      "增量保存第 88 批数据\n",
      "已处理 890 个文档块 (共 1437)\n",
      "增量保存第 89 批数据\n",
      "已处理 900 个文档块 (共 1437)\n",
      "增量保存第 90 批数据\n",
      "已处理 910 个文档块 (共 1437)\n",
      "增量保存第 91 批数据\n",
      "已处理 920 个文档块 (共 1437)\n",
      "增量保存第 92 批数据\n",
      "已处理 930 个文档块 (共 1437)\n",
      "增量保存第 93 批数据\n",
      "已处理 940 个文档块 (共 1437)\n",
      "增量保存第 94 批数据\n",
      "已处理 950 个文档块 (共 1437)\n",
      "增量保存第 95 批数据\n",
      "已处理 960 个文档块 (共 1437)\n",
      "增量保存第 96 批数据\n",
      "已处理 970 个文档块 (共 1437)\n",
      "增量保存第 97 批数据\n",
      "已处理 980 个文档块 (共 1437)\n",
      "增量保存第 98 批数据\n",
      "已处理 990 个文档块 (共 1437)\n",
      "增量保存第 99 批数据\n",
      "已处理 1000 个文档块 (共 1437)\n",
      "增量保存第 100 批数据\n",
      "已处理 1010 个文档块 (共 1437)\n",
      "增量保存第 101 批数据\n",
      "已处理 1020 个文档块 (共 1437)\n",
      "增量保存第 102 批数据\n",
      "已处理 1030 个文档块 (共 1437)\n",
      "增量保存第 103 批数据\n",
      "已处理 1040 个文档块 (共 1437)\n",
      "增量保存第 104 批数据\n",
      "已处理 1050 个文档块 (共 1437)\n",
      "增量保存第 105 批数据\n",
      "已处理 1060 个文档块 (共 1437)\n",
      "增量保存第 106 批数据\n",
      "已处理 1070 个文档块 (共 1437)\n",
      "增量保存第 107 批数据\n",
      "已处理 1080 个文档块 (共 1437)\n",
      "增量保存第 108 批数据\n",
      "已处理 1090 个文档块 (共 1437)\n",
      "增量保存第 109 批数据\n",
      "已处理 1100 个文档块 (共 1437)\n",
      "增量保存第 110 批数据\n",
      "已处理 1110 个文档块 (共 1437)\n",
      "增量保存第 111 批数据\n",
      "已处理 1120 个文档块 (共 1437)\n",
      "增量保存第 112 批数据\n",
      "已处理 1130 个文档块 (共 1437)\n",
      "增量保存第 113 批数据\n",
      "已处理 1140 个文档块 (共 1437)\n",
      "增量保存第 114 批数据\n",
      "已处理 1150 个文档块 (共 1437)\n",
      "增量保存第 115 批数据\n",
      "已处理 1160 个文档块 (共 1437)\n",
      "增量保存第 116 批数据\n",
      "已处理 1170 个文档块 (共 1437)\n",
      "增量保存第 117 批数据\n",
      "已处理 1180 个文档块 (共 1437)\n",
      "增量保存第 118 批数据\n",
      "已处理 1190 个文档块 (共 1437)\n",
      "增量保存第 119 批数据\n",
      "已处理 1200 个文档块 (共 1437)\n",
      "增量保存第 120 批数据\n",
      "已处理 1210 个文档块 (共 1437)\n",
      "增量保存第 121 批数据\n",
      "已处理 1220 个文档块 (共 1437)\n",
      "增量保存第 122 批数据\n",
      "已处理 1230 个文档块 (共 1437)\n",
      "增量保存第 123 批数据\n",
      "已处理 1240 个文档块 (共 1437)\n",
      "增量保存第 124 批数据\n",
      "已处理 1250 个文档块 (共 1437)\n",
      "增量保存第 125 批数据\n",
      "已处理 1260 个文档块 (共 1437)\n",
      "增量保存第 126 批数据\n",
      "已处理 1270 个文档块 (共 1437)\n",
      "增量保存第 127 批数据\n",
      "已处理 1280 个文档块 (共 1437)\n",
      "增量保存第 128 批数据\n",
      "已处理 1290 个文档块 (共 1437)\n",
      "增量保存第 129 批数据\n",
      "已处理 1300 个文档块 (共 1437)\n",
      "增量保存第 130 批数据\n",
      "已处理 1310 个文档块 (共 1437)\n",
      "增量保存第 131 批数据\n",
      "已处理 1320 个文档块 (共 1437)\n",
      "增量保存第 132 批数据\n",
      "已处理 1330 个文档块 (共 1437)\n",
      "增量保存第 133 批数据\n",
      "已处理 1340 个文档块 (共 1437)\n",
      "增量保存第 134 批数据\n",
      "已处理 1350 个文档块 (共 1437)\n",
      "增量保存第 135 批数据\n",
      "已处理 1360 个文档块 (共 1437)\n",
      "增量保存第 136 批数据\n",
      "已处理 1370 个文档块 (共 1437)\n",
      "增量保存第 137 批数据\n",
      "已处理 1380 个文档块 (共 1437)\n",
      "增量保存第 138 批数据\n",
      "已处理 1390 个文档块 (共 1437)\n",
      "增量保存第 139 批数据\n",
      "已处理 1400 个文档块 (共 1437)\n",
      "增量保存第 140 批数据\n",
      "已处理 1410 个文档块 (共 1437)\n",
      "增量保存第 141 批数据\n",
      "已处理 1420 个文档块 (共 1437)\n",
      "增量保存第 142 批数据\n",
      "已处理 1430 个文档块 (共 1437)\n",
      "增量保存第 143 批数据\n",
      "已处理 1437 个文档块 (共 1437)\n",
      "向量数据库最终保存至: ./backend/faiss_index_temp\n",
      "加载向量库成功\n"
     ]
    }
   ],
   "source": [
    "retrieve_model=Retrieve_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f05db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
